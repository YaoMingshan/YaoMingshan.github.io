<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">




<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

  <meta name="author" content="Evan">


  <meta name="subtitle" content="Hi! I'm Evan, an AI engineer passionate about ​​AI infra optimization​​ (training/inference acceleration, distributed systems), ​​algorithm efficiency​​ (quantization, pruning) and compiler tricks.">




<title>How to optimize reduce on gpu? | 1% faster matters</title>



<link rel="icon" href="/favicon.png">



<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/nprogress/nprogress.css">



<script src="/lib/jquery.min.js"></script>


<script src="/lib/iconify-icon.min.js"></script>


<script src="https://cdn.tailwindcss.com?plugins=typography"></script>
<script>
  tailwind.config = {
    darkMode: "class",
  };
</script>


<script src="/lib/nprogress/nprogress.js"></script>

<script>
  $(document).ready(() => {
    NProgress.configure({
      showSpinner: false,
    });
    NProgress.start();
    $("#nprogress .bar").css({
      background: "#de7441",
    });
    $("#nprogress .peg").css({
      "box-shadow": "0 0 2px #de7441, 0 0 4px #de7441",
    });
    $("#nprogress .spinner-icon").css({
      "border-top-color": "#de7441",
      "border-left-color": "#de7441",
    });
    setTimeout(function () {
      NProgress.done();
      $(".fade").removeClass("out");
    }, 800);
  });
</script>

<script>
  (function () {
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const setting = localStorage.getItem("hexo-color-scheme") || "auto";
    if (setting === "dark" || (prefersDark && setting !== "light"))
      document.documentElement.classList.toggle("dark", true);
    let isDark = document.documentElement.classList.contains("dark");
  })();

  $(document).ready(function () {
    // init icon
    const prefersDark =
      window.matchMedia &&
      window.matchMedia("(prefers-color-scheme: dark)").matches;
    const isDark = document.documentElement.classList.contains("dark");
    $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");

    function toggleGiscusTheme() {
      const isDark = document.documentElement.classList.contains("dark");
      const giscusFrame = document.querySelector("iframe.giscus-frame");
      if (giscusFrame) {
        giscusFrame.contentWindow.postMessage(
          {
            giscus: {
              setConfig: {
                theme: isDark ? "dark" : "light",
              },
            },
          },
          "https://giscus.app"
        );
      }
    }

    // toggle dark mode
    function toggleDark() {
      let isDark = document.documentElement.classList.contains("dark");
      const setting = localStorage.getItem("hexo-color-scheme") || "auto";
      isDark = !isDark;
      document.documentElement.classList.toggle("dark", isDark);
      $("#theme-icon").attr("icon", isDark ? "ri:moon-line" : "ri:sun-line");
      if (prefersDark === isDark) {
        localStorage.setItem("hexo-color-scheme", "auto");
      } else {
        localStorage.setItem("hexo-color-scheme", isDark ? "dark" : "light");
      }
      toggleGiscusTheme();
    }

    // listen dark mode change
    window
      .matchMedia("(prefers-color-scheme: dark)")
      .addEventListener("change", (e) => {
        const setting = localStorage.getItem("hexo-color-scheme") || "auto";
        if (setting === "auto") {
          document.documentElement.classList.toggle("dark", e.matches);
          $("#theme-icon").attr(
            "icon",
            e.matches ? "ri:moon-line" : "ri:sun-line"
          );
          toggleGiscusTheme();
        }
      });

    $("#toggle-dark").click((event) => {
      const isAppearanceTransition = document.startViewTransition && !window.matchMedia('(prefers-reduced-motion: reduce)').matches
      if (!isAppearanceTransition) {
        toggleDark()
        return
      }
      const x = event.clientX
      const y = event.clientY
      const endRadius = Math.hypot(
        Math.max(x, innerWidth - x),
        Math.max(y, innerHeight - y),
      )
      const transition = document.startViewTransition(async () => {
        toggleDark()
      })

      transition.ready
        .then(() => {
          const isDark = document.documentElement.classList.contains("dark")
          const clipPath = [
            `circle(0px at ${x}px ${y}px)`,
            `circle(${endRadius}px at ${x}px ${y}px)`,
          ]
          document.documentElement.animate(
            {
              clipPath: isDark
                ? [...clipPath].reverse()
                : clipPath,
            },
            {
              duration: 400,
              easing: 'ease-out',
              pseudoElement: isDark
                ? '::view-transition-old(root)'
                : '::view-transition-new(root)',
            },
          )
        })
    });
  });
</script>




<meta name="generator" content="Hexo 7.3.0"></head>
<body class="font-sans bg-white dark:bg-zinc-900 text-gray-700 dark:text-gray-200 relative">
  <header class="fixed w-full px-5 py-1 z-10 backdrop-blur-xl backdrop-saturate-150 border-b border-black/5">
  <div class="max-auto">
    <nav class="flex items-center text-base">
      <a href="/" class="group">
        <h2 class="font-medium tracking-tighterp text-l p-2">
          <img class="w-5 mr-2 inline-block transition-transform group-hover:rotate-[30deg]" id="logo" src="/images/logo.svg" alt="1% faster matters" />
          1% faster matters
        </h2>
      </a>
      <div id="header-title" class="opacity-0 md:ml-2 md:mt-[0.1rem] text-xs font-medium whitespace-nowrap overflow-hidden overflow-ellipsis">
        How to optimize reduce on gpu?
      </div>
      <div class="flex-1"></div>
      <div class="flex items-center gap-3">
        
          <a class="hidden sm:flex" href="/archives">Posts</a>
        
        
          
            <a class="w-5 h-5 hidden sm:flex" title="Github" target="_blank" rel="noopener" href="https://github.com/YaoMingshan">
              <iconify-icon width="20" icon="ri:github-line"></iconify-icon>
            </a>
          
        
        <a class="w-5 h-5 hidden sm:flex" title="Github" href="rss2.xml">
          <iconify-icon width="20" icon="ri:rss-line"></iconify-icon>
        </a>
        <a class="w-5 h-5" title="toggle theme" id="toggle-dark">
          <iconify-icon width="20" icon="" id="theme-icon"></iconify-icon>
        </a>
      </div>
      <div class="flex items-center justify-center gap-3 ml-3 sm:hidden">
        <span class="w-5 h-5" aria-hidden="true" role="img" id="open-menu">
          <iconify-icon width="20" icon="carbon:menu" ></iconify-icon>
        </span>
        <span class="w-5 h-5 hidden" aria-hidden="true" role="img" id="close-menu">
          <iconify-icon  width="20" icon="carbon:close" ></iconify-icon>
        </span>
      </div>
    </nav>
  </div>
</header>
<div id="menu-panel" class="h-0 overflow-hidden sm:hidden fixed left-0 right-0 top-12 bottom-0 z-10">
  <div id="menu-content" class="relative z-20 bg-white/80 px-6 sm:px-8 py-2 backdrop-blur-xl -translate-y-full transition-transform duration-300">
    <ul class="nav flex flex-col sm:flex-row text-sm font-medium">
      
        <li class="nav-portfolio sm:mx-2 border-b sm:border-0 border-black/5 last:border-0 hover:text-main">
          <a href="/archives" class="flex h-12 sm:h-auto items-center">Posts</a>
        </li>
      
    </ul>
  </div>
  <div class="mask bg-black/20 absolute inset-0"></div>
</div>

  <main class="pt-14">
    <!-- css -->

<link rel="stylesheet" href="/lib/fancybox/fancybox.min.css">


<link rel="stylesheet" href="/lib/tocbot/tocbot.min.css">

<!-- toc -->

  <!-- tocbot -->
<nav class="post-toc toc text-sm w-48 relative top-32 right-0 opacity-70 hidden lg:block" style="position: fixed !important;"></nav>


<section class="px-6 max-w-prose mx-auto md:px-0">
  <!-- header -->
  <header class="overflow-hidden pt-6 pb-6 md:pt-12">
    <div class="pt-4 md:pt-6">
      <h1 id="article-title" class="text-[2rem] font-bold leading-snug mb-4 md:mb-6 md:text-[2.6rem]">
        How to optimize reduce on gpu?
      </h1>
      <div>
        <section class="flex items-center gap-3 text-sm">
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="carbon-calendar" ></iconify-icon>
            <time>2025-07-20</time>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="ic:round-access-alarm" ></iconify-icon>
            <span>13 min</span>
          </span>
          <span class="text-gray-400">·</span>
          <span class="flex items-center gap-1">
            <iconify-icon width="18" icon="icon-park-outline:font-search" ></iconify-icon>
            <span>2.5k words</span>
          </span>
          
        </section>
      </div>
    </div>
  </header>
  <!-- content -->
  <article class="post-content prose m-auto slide-enter-content dark:prose-invert">
    <h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(torch) root@fdlmaadygeiyonja-snow-f6b585776-bsh9l:/data/coding# nvcc --version</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2024 NVIDIA Corporation</span><br><span class="line">Built on Tue_Oct_29_23:50:19_PDT_2024</span><br><span class="line">Cuda compilation tools, release 12.6, V12.6.85</span><br><span class="line">Build cuda_12.6.r12.6/compiler.35059454_0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br></pre></td><td class="code"><pre><span class="line">(torch) root@fdlmaadygeiyonja-snow-f6b585776-bsh9l:/data/coding# nvidia-smi -q</span><br><span class="line"></span><br><span class="line">==============NVSMI LOG==============</span><br><span class="line"></span><br><span class="line">Timestamp                                 : Sun Jul 20 14:40:46 2025</span><br><span class="line">Driver Version                            : 550.54.15</span><br><span class="line">CUDA Version                              : 12.4</span><br><span class="line"></span><br><span class="line">Attached GPUs                             : 1</span><br><span class="line">GPU 00000000:04:00.0</span><br><span class="line">    Product Name                          : Tesla P4</span><br><span class="line">    Product Brand                         : Tesla</span><br><span class="line">    Product Architecture                  : Pascal</span><br><span class="line">    Display Mode                          : Enabled</span><br><span class="line">    Display Active                        : Disabled</span><br><span class="line">    Persistence Mode                      : Disabled</span><br><span class="line">    Addressing Mode                       : N/A</span><br><span class="line">    MIG Mode</span><br><span class="line">        Current                           : N/A</span><br><span class="line">        Pending                           : N/A</span><br><span class="line">    Accounting Mode                       : Disabled</span><br><span class="line">    Accounting Mode Buffer Size           : 4000</span><br><span class="line">    Driver Model</span><br><span class="line">        Current                           : N/A</span><br><span class="line">        Pending                           : N/A</span><br><span class="line">    Serial Number                         : 1422019094907</span><br><span class="line">    GPU UUID                              : GPU-0c556aa8-3b59-7c23-b045-c2c66fc071d7</span><br><span class="line">    Minor Number                          : 1</span><br><span class="line">    VBIOS Version                         : 86.04.8C.00.10</span><br><span class="line">    MultiGPU Board                        : No</span><br><span class="line">    Board ID                              : 0x400</span><br><span class="line">    Board Part Number                     : 900-2G414-0000-001</span><br><span class="line">    GPU Part Number                       : 1BB3-895-A1</span><br><span class="line">    FRU Part Number                       : N/A</span><br><span class="line">    Module ID                             : 1</span><br><span class="line">    Inforom Version</span><br><span class="line">        Image Version                     : G414.0200.00.03</span><br><span class="line">        OEM Object                        : 1.1</span><br><span class="line">        ECC Object                        : 4.1</span><br><span class="line">        Power Management Object           : N/A</span><br><span class="line">    Inforom BBX Object Flush</span><br><span class="line">        Latest Timestamp                  : N/A</span><br><span class="line">        Latest Duration                   : N/A</span><br><span class="line">    GPU Operation Mode</span><br><span class="line">        Current                           : N/A</span><br><span class="line">        Pending                           : N/A</span><br><span class="line">    GPU C2C Mode                          : N/A</span><br><span class="line">    GPU Virtualization Mode</span><br><span class="line">        Virtualization Mode               : None</span><br><span class="line">        Host VGPU Mode                    : N/A</span><br><span class="line">        vGPU Heterogeneous Mode           : N/A</span><br><span class="line">    GPU Reset Status</span><br><span class="line">        Reset Required                    : No</span><br><span class="line">        Drain and Reset Recommended       : N/A</span><br><span class="line">    GSP Firmware Version                  : N/A</span><br><span class="line">    IBMNPU</span><br><span class="line">        Relaxed Ordering Mode             : N/A</span><br><span class="line">    PCI</span><br><span class="line">        Bus                               : 0x04</span><br><span class="line">        Device                            : 0x00</span><br><span class="line">        Domain                            : 0x0000</span><br><span class="line">        Base Classcode                    : 0x3</span><br><span class="line">        Sub Classcode                     : 0x2</span><br><span class="line">        Device Id                         : 0x1BB310DE</span><br><span class="line">        Bus Id                            : 00000000:04:00.0</span><br><span class="line">        Sub System Id                     : 0x11D810DE</span><br><span class="line">        GPU Link Info</span><br><span class="line">            PCIe Generation</span><br><span class="line">                Max                       : 3</span><br><span class="line">                Current                   : 1</span><br><span class="line">                Device Current            : 1</span><br><span class="line">                Device Max                : 3</span><br><span class="line">                Host Max                  : 3</span><br><span class="line">            Link Width</span><br><span class="line">                Max                       : 16x</span><br><span class="line">                Current                   : 8x</span><br><span class="line">        Bridge Chip</span><br><span class="line">            Type                          : N/A</span><br><span class="line">            Firmware                      : N/A</span><br><span class="line">        Replays Since Reset               : 0</span><br><span class="line">        Replay Number Rollovers           : 0</span><br><span class="line">        Tx Throughput                     : 0 KB/s</span><br><span class="line">        Rx Throughput                     : 0 KB/s</span><br><span class="line">        Atomic Caps Inbound               : N/A</span><br><span class="line">        Atomic Caps Outbound              : N/A</span><br><span class="line">    Fan Speed                             : N/A</span><br><span class="line">    Performance State                     : P8</span><br><span class="line">    Clocks Event Reasons</span><br><span class="line">        Idle                              : Active</span><br><span class="line">        Applications Clocks Setting       : Not Active</span><br><span class="line">        SW Power Cap                      : Not Active</span><br><span class="line">        HW Slowdown                       : Not Active</span><br><span class="line">            HW Thermal Slowdown           : Not Active</span><br><span class="line">            HW Power Brake Slowdown       : Not Active</span><br><span class="line">        Sync Boost                        : Not Active</span><br><span class="line">        SW Thermal Slowdown               : Not Active</span><br><span class="line">        Display Clock Setting             : Not Active</span><br><span class="line">    Sparse Operation Mode                 : N/A</span><br><span class="line">    FB Memory Usage</span><br><span class="line">        Total                             : 8192 MiB</span><br><span class="line">        Reserved                          : 82 MiB</span><br><span class="line">        Used                              : 0 MiB</span><br><span class="line">        Free                              : 8109 MiB</span><br><span class="line">    BAR1 Memory Usage</span><br><span class="line">        Total                             : 256 MiB</span><br><span class="line">        Used                              : 2 MiB</span><br><span class="line">        Free                              : 254 MiB</span><br><span class="line">    Conf Compute Protected Memory Usage</span><br><span class="line">        Total                             : 0 MiB</span><br><span class="line">        Used                              : 0 MiB</span><br><span class="line">        Free                              : 0 MiB</span><br><span class="line">    Compute Mode                          : Default</span><br><span class="line">    Utilization</span><br><span class="line">        Gpu                               : 0 %</span><br><span class="line">        Memory                            : 0 %</span><br><span class="line">        Encoder                           : 0 %</span><br><span class="line">        Decoder                           : 0 %</span><br><span class="line">        JPEG                              : N/A</span><br><span class="line">        OFA                               : N/A</span><br><span class="line">    Encoder Stats</span><br><span class="line">        Active Sessions                   : 0</span><br><span class="line">        Average FPS                       : 0</span><br><span class="line">        Average Latency                   : 0</span><br><span class="line">    FBC Stats</span><br><span class="line">        Active Sessions                   : 0</span><br><span class="line">        Average FPS                       : 0</span><br><span class="line">        Average Latency                   : 0</span><br><span class="line">    ECC Mode</span><br><span class="line">        Current                           : Disabled</span><br><span class="line">        Pending                           : Disabled</span><br><span class="line">    ECC Errors</span><br><span class="line">        Volatile</span><br><span class="line">            Single Bit      </span><br><span class="line">                Device Memory             : N/A</span><br><span class="line">                Register File             : N/A</span><br><span class="line">                L1 Cache                  : N/A</span><br><span class="line">                L2 Cache                  : N/A</span><br><span class="line">                Texture Memory            : N/A</span><br><span class="line">                Texture Shared            : N/A</span><br><span class="line">                CBU                       : N/A</span><br><span class="line">                Total                     : N/A</span><br><span class="line">            Double Bit      </span><br><span class="line">                Device Memory             : N/A</span><br><span class="line">                Register File             : N/A</span><br><span class="line">                L1 Cache                  : N/A</span><br><span class="line">                L2 Cache                  : N/A</span><br><span class="line">                Texture Memory            : N/A</span><br><span class="line">                Texture Shared            : N/A</span><br><span class="line">                CBU                       : N/A</span><br><span class="line">                Total                     : N/A</span><br><span class="line">        Aggregate</span><br><span class="line">            Single Bit      </span><br><span class="line">                Device Memory             : N/A</span><br><span class="line">                Register File             : N/A</span><br><span class="line">                L1 Cache                  : N/A</span><br><span class="line">                L2 Cache                  : N/A</span><br><span class="line">                Texture Memory            : N/A</span><br><span class="line">                Texture Shared            : N/A</span><br><span class="line">                CBU                       : N/A</span><br><span class="line">                Total                     : N/A</span><br><span class="line">            Double Bit      </span><br><span class="line">                Device Memory             : N/A</span><br><span class="line">                Register File             : N/A</span><br><span class="line">                L1 Cache                  : N/A</span><br><span class="line">                L2 Cache                  : N/A</span><br><span class="line">                Texture Memory            : N/A</span><br><span class="line">                Texture Shared            : N/A</span><br><span class="line">                CBU                       : N/A</span><br><span class="line">                Total                     : N/A</span><br><span class="line">    Retired Pages</span><br><span class="line">        Single Bit ECC                    : 0</span><br><span class="line">        Double Bit ECC                    : 0</span><br><span class="line">        Pending Page Blacklist            : No</span><br><span class="line">    Remapped Rows                         : N/A</span><br><span class="line">    Temperature</span><br><span class="line">        GPU Current Temp                  : 32 C</span><br><span class="line">        GPU T.Limit Temp                  : N/A</span><br><span class="line">        GPU Shutdown Temp                 : 94 C</span><br><span class="line">        GPU Slowdown Temp                 : 91 C</span><br><span class="line">        GPU Max Operating Temp            : N/A</span><br><span class="line">        GPU Target Temperature            : N/A</span><br><span class="line">        Memory Current Temp               : N/A</span><br><span class="line">        Memory Max Operating Temp         : N/A</span><br><span class="line">    GPU Power Readings</span><br><span class="line">        Power Draw                        : 6.61 W</span><br><span class="line">        Current Power Limit               : 75.00 W</span><br><span class="line">        Requested Power Limit             : 75.00 W</span><br><span class="line">        Default Power Limit               : 75.00 W</span><br><span class="line">        Min Power Limit                   : 60.00 W</span><br><span class="line">        Max Power Limit                   : 75.00 W</span><br><span class="line">    GPU Memory Power Readings </span><br><span class="line">        Power Draw                        : N/A</span><br><span class="line">    Module Power Readings</span><br><span class="line">        Power Draw                        : N/A</span><br><span class="line">        Current Power Limit               : N/A</span><br><span class="line">        Requested Power Limit             : N/A</span><br><span class="line">        Default Power Limit               : N/A</span><br><span class="line">        Min Power Limit                   : N/A</span><br><span class="line">        Max Power Limit                   : N/A</span><br><span class="line">    Clocks</span><br><span class="line">        Graphics                          : 455 MHz</span><br><span class="line">        SM                                : 455 MHz</span><br><span class="line">        Memory                            : 405 MHz</span><br><span class="line">        Video                             : 455 MHz</span><br><span class="line">    Applications Clocks</span><br><span class="line">        Graphics                          : 885 MHz</span><br><span class="line">        Memory                            : 3003 MHz</span><br><span class="line">    Default Applications Clocks</span><br><span class="line">        Graphics                          : 885 MHz</span><br><span class="line">        Memory                            : 3003 MHz</span><br><span class="line">    Deferred Clocks</span><br><span class="line">        Memory                            : N/A</span><br><span class="line">    Max Clocks</span><br><span class="line">        Graphics                          : 1531 MHz</span><br><span class="line">        SM                                : 1531 MHz</span><br><span class="line">        Memory                            : 3003 MHz</span><br><span class="line">        Video                             : 1379 MHz</span><br><span class="line">    Max Customer Boost Clocks</span><br><span class="line">        Graphics                          : 1113 MHz</span><br><span class="line">    Clock Policy</span><br><span class="line">        Auto Boost                        : N/A</span><br><span class="line">        Auto Boost Default                : N/A</span><br><span class="line">    Voltage</span><br><span class="line">        Graphics                          : N/A</span><br><span class="line">    Fabric</span><br><span class="line">        State                             : N/A</span><br><span class="line">        Status                            : N/A</span><br><span class="line">        CliqueId                          : N/A</span><br><span class="line">        ClusterUUID                       : N/A</span><br><span class="line">        Health</span><br><span class="line">            Bandwidth                     : N/A</span><br><span class="line">    Processes                             : None</span><br><span class="line"></span><br><span class="line">(torch) root@fdlmaadygeiyonja-snow-f6b585776-bsh9l:/data/coding# </span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>GPU</th>
<th>SM total</th>
<th>cuda core total</th>
<th>Bandwidth</th>
</tr>
</thead>
<tbody><tr>
<td>P4</td>
<td>20</td>
<td>2560</td>
<td>192GB&#x2F;s</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h1 id="Key-Optimization"><a href="#Key-Optimization" class="headerlink" title="Key Optimization"></a>Key Optimization</h1><ul>
<li><p><input checked="" disabled="" type="checkbox"> 
Memory coalescing，提高带宽利用率</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
避免divergence，特别核心循环内</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
避免share memory bank conflict，stride访问容易出现，而linear访问是天然无冲突的</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
loop unroll，减少循环控制开销，如warp reduce的展开；非warp reduce的展开</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
增加每个线程处理的数据量，同样增加每个block处理的数据量；而不是采取增加block数量和线程数量。因为有利于latency掩藏、减少launch overhead等开销。</p>
<p><img src="/image/How-to-optimize-reduce-on-gpu/1752997141379.png" alt="1752997141379"></p>
</li>
</ul>
<blockquote>
<p><strong>Algorithm Cascading（算法级联）</strong> 是一种<strong>混合并行与串行计算</strong>的优化策略，核心思想是通过<strong>分阶段处理</strong>来平衡并行效率和资源成本。它的名字来源于“级联”（像瀑布一样分阶段流动），即先让每个处理器&#x2F;线程<strong>串行处理局部数据</strong>，再<strong>并行整合全局结果</strong>。</p>
<hr>
<h3 id="1-为什么需要-Algorithm-Cascading？"><a href="#1-为什么需要-Algorithm-Cascading？" class="headerlink" title="1. 为什么需要 Algorithm Cascading？"></a>1. <strong>为什么需要 Algorithm Cascading？</strong></h3><ul>
<li><p><strong>直接纯并行的问题</strong>：</p>
<p>如果对 N 个数据启动 N 个线程并行归约（如求和），虽然时间从 O(N) 降到 O(log N)，但总成本（线程数 × 时间）是 <strong>O(N log N)</strong>，反而比串行算法（O(N)）更差。</p>
</li>
<li><p><strong>目标</strong>：</p>
<p>保持总成本 <strong>O(N)</strong>（和串行相同），但通过并行加速实际运行时间。</p>
</li>
</ul>
<hr>
<h3 id="2-具体如何实现？"><a href="#2-具体如何实现？" class="headerlink" title="2. 具体如何实现？"></a>2. <strong>具体如何实现？</strong></h3><p>分两阶段（以归约求和为例）：</p>
<h4 id="阶段1：局部串行计算（Sequential-Work）"><a href="#阶段1：局部串行计算（Sequential-Work）" class="headerlink" title="阶段1：局部串行计算（Sequential Work）"></a><strong>阶段1：局部串行计算（Sequential Work）</strong></h4><ul>
<li><p><strong>每个线程处理 log N 个数据</strong>（而非1个）。</p>
<p><em>例如：N&#x3D;1024，log N&#x3D;10 → 只需 1024&#x2F;10≈102 个线程，每个线程用循环顺序求10个数的和。</em></p>
</li>
<li><p><strong>作用</strong>：</p>
<p>减少线程数量（从 O(N) 降到 O(N&#x2F;log N)），避免资源浪费。</p>
</li>
</ul>
<h4 id="阶段2：全局并行整合（Parallel-Cooperation）"><a href="#阶段2：全局并行整合（Parallel-Cooperation）" class="headerlink" title="阶段2：全局并行整合（Parallel Cooperation）"></a><strong>阶段2：全局并行整合（Parallel Cooperation）</strong></h4><ul>
<li><p>对阶段1的局部结果（共 O(N&#x2F;log N) 个）进行<strong>树形并行归约</strong>，耗时 O(log N)。</p>
<p><em>例如：102 个局部和，用并行归约合并。</em></p>
</li>
<li><p><strong>总成本</strong>：</p>
<p>(N&#x2F;log N 线程) × (log N 时间) &#x3D; <strong>O(N)</strong>（成本最优）。</p>
</li>
</ul>
<hr>
<h3 id="3-在-GPU-CUDA-中的实际应用"><a href="#3-在-GPU-CUDA-中的实际应用" class="headerlink" title="3. 在 GPU&#x2F;CUDA 中的实际应用"></a>3. <strong>在 GPU&#x2F;CUDA 中的实际应用</strong></h3><ul>
<li><p><strong>线程分配优化</strong>：</p>
<p>GPU 线程块（Block）的线程数通常设为 256 或 512，远小于数据量 N。每个线程通过循环处理多个数据（阶段1），再用共享内存归约（阶段2）。</p>
</li>
<li><p><strong>性能提升</strong>：</p>
<p>减少线程调度开销，提高计算密度，显著加速实际运行时间。</p>
</li>
</ul>
<hr>
<h3 id="4-类比理解"><a href="#4-类比理解" class="headerlink" title="4. 类比理解"></a>4. <strong>类比理解</strong></h3><p>把算法级联想成<strong>小组分工</strong>：</p>
<ol>
<li><p><strong>组内串行</strong>：每组（线程）先内部讨论汇总意见（局部计算）。</p>
</li>
<li><p><strong>组间并行</strong>：各组代表快速整合最终结论（全局归约）。</p>
<p>这样比所有人（O(N) 线程）同时发言更高效！</p>
</li>
</ol>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Algorithm Cascading 的本质是：</p>
<p><strong>用串行换线程，用并行换时间</strong>，最终在<strong>不增加总成本</strong>的前提下，最大化实际加速比。它是并行算法设计中的经典优化手段，在 GPU 计算、MapReduce 等场景广泛应用。</p>
</blockquote>
<h1 id="Reduce-Kernel"><a href="#Reduce-Kernel" class="headerlink" title="Reduce Kernel"></a>Reduce Kernel</h1><p>按照上述的guildline，带宽利用率可以达到80%，但应该不是极限，怎样才能达到满带宽利用率？</p>
<ul>
<li><input disabled="" type="checkbox"> 单个 thread内unroll更多次load，通过静态的传unroll times，或者float4类型</li>
<li><input disabled="" type="checkbox"> 用__shfl_xor做warp reduce，见 <code>https://github.com/DefTruth/CUDA-Learn-Notes/blob/main/kernels/reduce/block_all_reduce.cu</code></li>
<li><input disabled="" type="checkbox"> 通过更细的profile数据决定优化方向</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> TOTAL_SIZE = <span class="number">1</span> &lt;&lt; <span class="number">30</span>; <span class="comment">// 输入数组大小</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BLOCK_SIZE 128</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GRID_SIZE 20</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CUDA_CHECK(call) \</span></span><br><span class="line"><span class="meta">do &#123; \</span></span><br><span class="line"><span class="meta">    cudaError_t err = (call); \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (err != cudaSuccess) &#123; \</span></span><br><span class="line"><span class="meta">        fprintf(stderr, <span class="string">&quot;CUDA error at %s:%d - %s\n&quot;</span>, \</span></span><br><span class="line"><span class="meta">                __FILE__, __LINE__, cudaGetErrorString(err)); \</span></span><br><span class="line"><span class="meta">        exit(EXIT_FAILURE); \</span></span><br><span class="line"><span class="meta">    &#125; \</span></span><br><span class="line"><span class="meta">&#125; while(0)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> KERNEL_CHECK() \</span></span><br><span class="line"><span class="meta">do &#123; \</span></span><br><span class="line"><span class="meta">    cudaError_t err = cudaGetLastError(); \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (err != cudaSuccess) &#123; \</span></span><br><span class="line"><span class="meta">        fprintf(stderr, <span class="string">&quot;Kernel error: %s\n&quot;</span>, cudaGetErrorString(err)); \</span></span><br><span class="line"><span class="meta">        exit(EXIT_FAILURE); \</span></span><br><span class="line"><span class="meta">    &#125; \</span></span><br><span class="line"><span class="meta">&#125; while(0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span>  <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="type">float</span> *sdata, <span class="type">unsigned</span> <span class="type">int</span> tid)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">64</span>) sdata[tid] += sdata[tid + <span class="number">32</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">32</span>) sdata[tid] += sdata[tid + <span class="number">16</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">16</span>) sdata[tid] += sdata[tid + <span class="number">8</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">8</span>) sdata[tid] += sdata[tid + <span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">4</span>) sdata[tid] += sdata[tid + <span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">2</span>) sdata[tid] += sdata[tid + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce_kernel</span><span class="params">(<span class="type">float</span> *d_in, <span class="type">float</span> *d_out, <span class="type">int</span> total_size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">float</span> sdata[];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i = blockIdx.x * blockDim.x * <span class="number">2</span> + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> gridSize = blockDim.x * <span class="number">2</span> * gridDim.x;</span><br><span class="line">    sdata[tid] = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (i &lt; total_size) &#123;</span><br><span class="line">        sdata[tid] += d_in[i] + d_in[i + blockDim.x];</span><br><span class="line">        i += gridSize;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 归约共享内存中的数据</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">512</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">256</span>)  sdata[tid] += sdata[tid + <span class="number">256</span>];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">256</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">128</span>) sdata[tid] += sdata[tid + <span class="number">128</span>];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt;= <span class="number">128</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; <span class="number">64</span>) sdata[tid] += sdata[tid + <span class="number">64</span>];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid &lt; <span class="number">32</span>) <span class="built_in">warpReduce</span>&lt;blockSize&gt;(sdata, tid);</span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) d_out[blockIdx.x] = sdata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 主机函数：执行归约操作</span></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">reduce</span><span class="params">(<span class="type">float</span> *h_in, <span class="type">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> *d_in, *d_out;</span><br><span class="line">    <span class="type">float</span> h_out;</span><br><span class="line">    <span class="type">int</span> num_blocks = (size + BLOCK_SIZE - <span class="number">1</span>) / BLOCK_SIZE;</span><br><span class="line">    <span class="type">float</span> *h_out_array = (<span class="type">float</span> *)<span class="built_in">malloc</span>(num_blocks * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;d_in, size * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;d_out, num_blocks * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_in, h_in, size * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice);</span><br><span class="line">  </span><br><span class="line">    reduce_kernel&lt;BLOCK_SIZE&gt;&lt;&lt;&lt;<span class="number">320</span>, BLOCK_SIZE, <span class="function">BLOCK_SIZE * <span class="title">sizeof</span><span class="params">(<span class="type">float</span>)</span><span class="comment">/*share mem size*/</span>&gt;&gt;&gt;<span class="params">(d_in, d_out, size)</span></span>;</span><br><span class="line">    <span class="built_in">KERNEL_CHECK</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(h_out_array, d_out, num_blocks * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_in);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_out);</span><br><span class="line">    <span class="comment">// 在主机上对每个块的结果进行最终归约</span></span><br><span class="line">    h_out = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_blocks; i++) &#123;</span><br><span class="line">        h_out += h_out_array[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">free</span>(h_out_array);</span><br><span class="line">    <span class="keyword">return</span> h_out;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> N = TOTAL_SIZE; <span class="comment">// 1M元素</span></span><br><span class="line">    <span class="type">float</span> *h_in = (<span class="type">float</span> *)<span class="built_in">malloc</span>(N * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 初始化输入数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        h_in[i] = <span class="number">1.0</span>; <span class="comment">// 简单起见，所有元素设为1</span></span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 执行归约操作</span></span><br><span class="line">    <span class="type">float</span> sum = <span class="built_in">reduce</span>(h_in, N);</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Sum of %d elements: %f\n&quot;</span>, N, sum);</span><br><span class="line">  </span><br><span class="line">    <span class="built_in">free</span>(h_in);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvcc -<span class="built_in">arch</span>=compute_61 -code=sm_61 -o reduce reduce.cu</span><br><span class="line">nvcc -O0 -g -G -<span class="built_in">arch</span>=sm_61 -o reduce reduce.cu</span><br><span class="line">nvcc -O3 -Xcompiler -O3 --generate-code <span class="built_in">arch</span>=compute_61,code=sm_61 -o reduce reduce.cu</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本性能分析（不需要输出文件）</span></span><br><span class="line">nvprof ./reduce</span><br><span class="line"><span class="comment"># 查看带宽/算力利用率等</span></span><br><span class="line">nvprof --metrics all ./reduce</span><br><span class="line">--metrics gld_efficiency</span><br><span class="line">--metrics shared_efficiency</span><br><span class="line">--metrics branch_efficiency</span><br><span class="line">--metrics achieved_occupancy</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nvprof --print-gpu-trace ./a.out</span><br><span class="line"></span><br><span class="line">==6916== NVPROF is profiling process 6916, <span class="built_in">command</span>: ./reduce</span><br><span class="line">Sum of 1073741824 elements: 1073741824.000000</span><br><span class="line">==6916== Profiling application: ./reduce</span><br><span class="line">==6916== Profiling result:</span><br><span class="line">   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name</span><br><span class="line">406.75ms  737.20ms                    -               -         -         -         -  4.0000GB  5.4260GB/s    Pageable      Device     Tesla P4 (0)         1         7  [CUDA memcpy HtoD]</span><br><span class="line">1.14403s  56.695ms        (4194304 1 1)       (256 1 1)        10        0B  1.0000KB         -           -           -           -     Tesla P4 (0)         1         7  void reduce_kernel&lt;unsigned int=256&gt;(<span class="built_in">float</span>*, <span class="built_in">float</span>*, int) [128]</span><br><span class="line">1.20073s  11.884ms                    -               -         -         -         -  16.000MB  1.3148GB/s      Device    Pageable     Tesla P4 (0)         1         7  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.</span><br><span class="line">SSMem: Static shared memory allocated per CUDA block.</span><br><span class="line">DSMem: Dynamic shared memory allocated per CUDA block.</span><br><span class="line">SrcMemType: The <span class="built_in">type</span> of <span class="built_in">source</span> memory accessed by memory operation/copy</span><br><span class="line">DstMemType: The <span class="built_in">type</span> of destination memory accessed by memory operation/copy</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ncu --metrics sm__warps_active.avg.pct_of_peak_sustained,sm__global_throughput.avg.pct_of_peak_sustained ./reduce</span><br></pre></td></tr></table></figure>

<p>查看汇编</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cuobjdump -sass ./reduce &gt; asm.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>


<h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><p>nvdia: Optimizing Parallel Reduction in CUDA<a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf"> link</a></p>
<h1 id="Plan"><a href="#Plan" class="headerlink" title="Plan"></a>Plan</h1><ul>
<li><input checked="" disabled="" type="checkbox"> Reduce</li>
<li><input disabled="" type="checkbox"> Gemm opt</li>
<li><input disabled="" type="checkbox"> Sort opt</li>
<li><input disabled="" type="checkbox"> Flashattention opt</li>
</ul>

  </article>
  <!-- tag -->
  <div class="mt-12 pt-6 border-t border-gray-200">
    
  </div>
  <!-- prev and next -->
  <div class="flex justify-between mt-12 pt-6 border-t border-gray-200">
    <div>
      
    </div>
    <div>
      
        <a href="/2025/07/19/mac-github-page-setup/" class="text-sm text-gray-400 hover:text-gray-500 flex justify-center">
          Github pages setup on Mac
          <iconify-icon width="20" icon="ri:arrow-right-s-line" data-inline="false"></iconify-icon>
        </a>
      
    </div>
  </div>
  <!-- comment -->
  <div class="article-comments mt-12">
    
<script
  src="https://giscus.app/client.js"
  data-repo=""
  data-repo-id=""
  data-category=""
  data-category-id=""
  data-mapping=""
  data-strict="0"
  data-reactions-enabled="1"
  data-emit-metadata="0"
  data-input-position="bottom"
  data-theme="preferred_color_scheme"
  data-lang="zh-CN"
  crossorigin="anonymous"
  async
></script>
<script>
  window.onload = function () {
    console.log("giscus loaded");
    const isDark = document.documentElement.classList.contains("dark");
    const giscusFrame = document.querySelector("iframe.giscus-frame");
    giscusFrame.contentWindow.postMessage(
      {
        giscus: {
          setConfig: {
            theme: isDark ? "dark" : "light",
          },
        },
      },
      "https://giscus.app"
    );
  };
</script>


  </div>
</section>
<!-- js inspect -->

<script src="/lib/clipboard.min.js"></script>


<script async src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
  $(document).ready(() => {
    const maraidConfig = {
      theme: "default",
      logLevel: 3,
      flowchart: { curve: "linear" },
      gantt: { axisFormat: "%m/%d/%Y" },
      sequence: { actorMargin: 50 },
    };
    mermaid.initialize(maraidConfig);
  });
</script>



<script src="/lib/fancybox/fancybox.umd.min.js"></script>

<script>
  $(document).ready(() => {
    $('.post-content').each(function(i){
      $(this).find('img').each(function(){
        if ($(this).parent().hasClass('fancybox') || $(this).parent().is('a')) return;
        var alt = this.alt;
        if (alt) $(this).after('<span class="fancybox-alt">' + alt + '</span>');
        $(this).wrap('<a class="fancybox-img" href="' + this.src + '" data-fancybox=\"gallery\" data-caption="' + alt + '"></a>')
      });
      $(this).find('.fancybox').each(function(){
        $(this).attr('rel', 'article' + i);
      });
    });

    Fancybox.bind('[data-fancybox="gallery"]', {
        // options
    })
  })
</script>

<!-- tocbot begin -->

<script src="/lib/tocbot/tocbot.min.js"></script>

<script>
  $(document).ready(() => {
      tocbot.init({
        // Where to render the table of contents.
        tocSelector: '.post-toc',
        // Where to grab the headings to build the table of contents.
        contentSelector: '.post-content',
        // Which headings to grab inside of the contentSelector element.
        headingSelector: 'h1, h2, h3',
        // For headings inside relative or absolute positioned containers within content.
        hasInnerContainers: true,
    });
  })
</script>
<!-- tocbot end -->


  </main>
  <footer class="flex flex-col h-40 items-center justify-center text-gray-400 text-sm">
  <!-- busuanzi -->
  
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!-- Busuanzi Analytics -->
<div class="flex items-center gap-2">
  <span>Visitors</span>
  <span id="busuanzi_value_site_uv"></span>
  <span>Page Views</span>
  <span id="busuanzi_value_site_pv"></span>
</div>
<!-- End Busuanzi Analytics -->


  <!-- copyright -->
  <div class="flex items-center gap-2">
    <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" style="color: inherit;">CC BY-NC-SA 4.0</a>
    <span>© 2022</span>
    <iconify-icon width="18" icon="emojione-monotone:maple-leaf" ></iconify-icon>
    <a href="https://github.com/xbmlz" target="_blank" rel="noopener noreferrer">xbmlz</a>
  </div>
  <!-- powered by -->
  <div class="flex items-center gap-2">
    <span>Powered by</span>
    <a href="https://hexo.io/" target="_blank" rel="noopener noreferrer">Hexo</a>
    <span>&</span>
    <a href="https://github.com/xbmlz/hexo-theme-maple" target="_blank" rel="noopener noreferrer">Maple</a>
  </div>

</footer>

  <div class="back-to-top box-border fixed right-6 z-1024 -bottom-20 rounded py-1 px-1 bg-slate-900 opacity-60 text-white cursor-pointer text-center dark:bg-slate-600">
    <span class="flex justify-center items-center text-sm">
      <iconify-icon width="18" icon="ion:arrow-up-c" id="go-top"></iconify-icon>
      <span id="scrollpercent"><span>0</span> %</span>
    </span>
  </div>
  
<script src="/js/main.js"></script>


  <script>
    $(document).ready(function () {
      const mapleCount = "10";
      const speed = "0.5";
      const mapleEl = document.getElementById("maple");
      const maples = Array.from({ length: mapleCount }).map(() => {
        const maple = document.createElement("div");
        const scale = Math.random() * 0.5 + 0.5;
        const offset = Math.random() * 2 - 1;
        const x = Math.random() * mapleEl.clientWidth;
        const y = -Math.random() * mapleEl.clientHeight;
        const duration = 10 / speed;
        const delay = -duration;
        maple.className = "maple";
        maple.style.width = `${24 * scale}px`;
        maple.style.height = `${24 * scale}px`;
        maple.style.left = `${x}px`;
        maple.style.top = `${y}px`;
        maple.style.setProperty("--maple-fall-offset", offset);
        maple.style.setProperty("--maple-fall-height", `${Math.abs(y) + mapleEl.clientHeight}px`);
        maple.style.animation = `fall ${duration}s linear infinite`;
        maple.style.animationDelay = `${delay}s`;
        mapleEl.appendChild(maple)
        return maple
      })
    });
  </script>
  


  <div class="fixed top-0 bottom-0 left-0 right-0 pointer-events-none print:hidden" id="maple"></div>
</body>

</html>
